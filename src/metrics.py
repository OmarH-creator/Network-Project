"""
TinyTelemetry v1.0 Metrics Collection and Analysis

This module provides metrics calculation, CPU profiling, and statistical analysis
for telemetry protocol performance evaluation.
"""

import csv
import time
import json
import statistics
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

try:
    from .protocol import HEADER_SIZE, READING_SIZE
except ImportError:
    from protocol import HEADER_SIZE, READING_SIZE


@dataclass
class MetricsResult:
    """Container for calculated metrics."""
    bytes_per_report: float
    packets_received: int
    packets_sent: int
    duplicate_rate: float
    sequence_gap_count: int
    cpu_ms_per_report: Optional[float] = None


class MetricsCalculator:
    """
    Calculate performance metrics from CSV log files.
    
    Metrics calculated:
    - bytes_per_report: Average bytes per reading (header + payload)
    - packets_received: Count of non-duplicate packets
    - duplicate_rate: Fraction of duplicate packets
    - sequence_gap_count: Total missing packets detected
    """
    
    def __init__(self):
        """Initialize the metrics calculator."""
        pass
    
    def calculate_from_csv(self, csv_file: str) -> MetricsResult:
        """
        Calculate metrics from a CSV log file.
        
        Args:
            csv_file: Path to CSV log file generated by CollectorServer
            
        Returns:
            MetricsResult object with calculated metrics
            
        Raises:
            FileNotFoundError: If CSV file doesn't exist
            ValueError: If CSV file is malformed or empty
        """
        csv_path = Path(csv_file)
        
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV file not found: {csv_file}")
        
        # Initialize counters
        total_packets = 0
        duplicate_count = 0
        non_duplicate_count = 0
        total_readings = 0
        total_gap_count = 0
        
        # Read CSV file
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                total_packets += 1
                
                # Parse duplicate flag
                is_duplicate = row['duplicate_flag'].lower() == 'true'
                
                if is_duplicate:
                    duplicate_count += 1
                else:
                    non_duplicate_count += 1
                    
                    # Count readings (only for non-duplicates)
                    reading_count = int(row['reading_count'])
                    total_readings += reading_count
                
                # Sum gap counts
                gap_size = int(row['gap_size'])
                total_gap_count += gap_size
        
        if total_packets == 0:
            raise ValueError("CSV file contains no data packets")
        
        # Calculate duplicate rate
        duplicate_rate = duplicate_count / total_packets if total_packets > 0 else 0.0
        
        # Calculate bytes_per_report metric
        # This metric measures bandwidth efficiency by calculating the average
        # number of bytes required to transmit one sensor reading.
        # 
        # Formula: bytes_per_report = total_bytes / total_readings
        # 
        # Packet structure:
        # - Header: 12 bytes (fixed)
        # - Count: 1 byte (for DATA messages)
        # - Readings: 5 bytes each (sensor_type + float value)
        # 
        # Examples:
        # - Non-batched (1 reading/packet): (12 + 1 + 5) / 1 = 18 bytes per reading
        # - Batched (10 readings/packet): (12 + 1 + 50) / 10 = 6.3 bytes per reading
        # - Maximum batch (37 readings): (12 + 1 + 185) / 37 = 5.35 bytes per reading
        #
        # Lower values indicate better bandwidth efficiency.
        if total_readings > 0:
            # Calculate total bytes sent for non-duplicate packets
            # We only count non-duplicates to avoid inflating the metric
            total_bytes = 0
            
            # Re-read CSV to calculate total bytes
            with open(csv_path, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    is_duplicate = row['duplicate_flag'].lower() == 'true'
                    if not is_duplicate:
                        reading_count = int(row['reading_count'])
                        if reading_count > 0:  # DATA message
                            # DATA packet: header + count byte + readings
                            packet_size = HEADER_SIZE + 1 + (reading_count * READING_SIZE)
                        else:  # HEARTBEAT message
                            # HEARTBEAT packet: header only (no readings to count)
                            packet_size = HEADER_SIZE
                        total_bytes += packet_size
            
            # Calculate average bytes per reading
            bytes_per_report = total_bytes / total_readings if total_readings > 0 else 0.0
        else:
            # Only heartbeats, no readings to measure
            bytes_per_report = 0.0
        
        # Create result
        result = MetricsResult(
            bytes_per_report=bytes_per_report,
            packets_received=non_duplicate_count,
            packets_sent=total_packets + total_gap_count,  # Received + missing
            duplicate_rate=duplicate_rate,
            sequence_gap_count=total_gap_count
        )
        
        return result


def measure_cpu_time(func, *args, **kwargs) -> tuple:
    """
    Measure CPU time for a function execution.
    
    Args:
        func: Function to measure
        *args: Positional arguments for func
        **kwargs: Keyword arguments for func
        
    Returns:
        Tuple of (result, cpu_time_seconds)
    """
    start_cpu = time.process_time()
    result = func(*args, **kwargs)
    end_cpu = time.process_time()
    
    cpu_time = end_cpu - start_cpu
    
    return result, cpu_time


class StatisticalAnalyzer:
    """
    Perform statistical analysis on metric values.
    
    Calculates min, median, and max for repeated measurements.
    """
    
    def __init__(self):
        """Initialize the statistical analyzer."""
        pass
    
    def calculate_statistics(self, values: List[float]) -> Dict[str, float]:
        """
        Calculate statistical summary of values.
        
        Args:
            values: List of numeric values
            
        Returns:
            Dictionary with 'min', 'median', 'max' keys
            
        Raises:
            ValueError: If values list is empty
        """
        if not values:
            raise ValueError("Cannot calculate statistics on empty list")
        
        return {
            'min': min(values),
            'median': statistics.median(values),
            'max': max(values)
        }


def save_metrics_json(metrics_data: Dict[str, Any], output_file: str) -> None:
    """
    Save metrics to JSON file with pretty printing.
    
    Args:
        metrics_data: Dictionary containing test scenario, duration, metrics, and statistics
        output_file: Path to output JSON file
        
    The metrics_data dictionary should have the structure:
    {
        'test_scenario': str,
        'duration_seconds': int,
        'reporting_interval': int,
        'batch_size': int,
        'metrics': {
            'bytes_per_report': float,
            'packets_received': int,
            'packets_sent': int,
            'duplicate_rate': float,
            'sequence_gap_count': int,
            'cpu_ms_per_report': float (optional)
        },
        'statistics': {
            'min': {...},
            'median': {...},
            'max': {...}
        } (optional)
    }
    """
    output_path = Path(output_file)
    
    # Create output directory if it doesn't exist
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Write JSON with pretty printing
    with open(output_path, 'w') as f:
        json.dump(metrics_data, f, indent=2)
    
    print(f"Metrics saved to: {output_file}")
